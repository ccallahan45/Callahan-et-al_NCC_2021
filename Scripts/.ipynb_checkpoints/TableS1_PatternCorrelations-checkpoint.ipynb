{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nino3.4-based spatial pattern correlations\n",
    "#### Christopher Callahan\n",
    "#### Christopher.W.Callahan.GR@dartmouth.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mechanics\n",
    "Read in dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from matplotlib.patches import Polygon\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_tas = \"/dartfs-hpc/rc/lab/C/CMIG/ccallahan/ENSO/RAW_DATA/tas/\"\n",
    "loc_nino34 = \"/dartfs-hpc/rc/lab/C/CMIG/ccallahan/ENSO/NINO_INDICES/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set models list, experiments, colors, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames_fig = ['CCSM3 abrupt 2x','CCSM3 abrupt 4x','CCSM3 abrupt 8x', \\\n",
    "    'CESM1.0.4 abrupt 2x','CESM1.0.4 abrupt 4x','CESM1.0.4 abrupt 8x', 'CNRM-CM6.1 abrupt4x', \\\n",
    "    'GFDL-CM3 1pct 2x','GFDL-ESM2M 1pct 2x','GISS-E2-R 1pct 4x', \\\n",
    "    'GISS-E2-R abrupt 4x','HadCM3L abrupt 2x','HadCM3L abrupt 4x', \\\n",
    "    'HadCM3L abrupt 6x','HadCM3L abrupt 8x','IPSL-CM5A-LR abrupt 4x', \\\n",
    "    'MIROC3.2 1pct 2x','MIROC3.2 1pct 4x','MPIESM-1.2 abrupt 2x', \\\n",
    "    'MPIESM-1.2 abrupt 4x','MPIESM-1.2 abrupt 8x'] #,'MPIESM-1.1 abrupt4x']\n",
    "\n",
    "modelnames_file = ['CCSM3_abrupt2x','CCSM3_abrupt4x','CCSM3_abrupt8x', \\\n",
    "    'CESM104_abrupt2x','CESM104_abrupt4x','CESM104_abrupt8x', \\\n",
    "    'CNRMCM61_abrupt4x','GFDLCM3_1pct2x','GFDLESM2M_1pct2x','GISSE2R_1pct4x', \\\n",
    "    'GISSE2R_abrupt4x','HadCM3L_abrupt2x','HadCM3L_abrupt4x', \\\n",
    "    'HadCM3L_abrupt6x','HadCM3L_abrupt8x','IPSLCM5A_abrupt4x', \\\n",
    "    'MIROC32_1pct2x','MIROC32_1pct4x','MPIESM12_abrupt2x', \\\n",
    "    'MPIESM12_abrupt4x','MPIESM12_abrupt8x'] #,'MPIESM11_abrupt4x']\n",
    "\n",
    "runtype = ['abrupt','abrupt','abrupt','abrupt','abrupt','abrupt','lin','lin','lin', \\\n",
    "            'abrupt','abrupt','abrupt','abrupt','abrupt','abrupt', \\\n",
    "            'lin','lin','abrupt','abrupt','abrupt','abrupt'] #,'abrupt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write function for hiding top and right axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hide_right_and_top(axis):\n",
    "    \n",
    "    # This function hides the right and top axes\n",
    "    # of a given axis object\n",
    "    # For purely aesthetic purposes\n",
    "    \n",
    "    # Hide the right and top spines\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "    # Only show ticks on the left and bottom spines\n",
    "    axis.yaxis.set_ticks_position('left')\n",
    "    axis.xaxis.set_ticks_position('bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern correlation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN CORRELATION\n",
    "# Christopher Callahan\n",
    "# April 2019\n",
    "# Christopher.W.Callahan.GR@dartmouth.edu\n",
    "# Based on NCL's pattern correlation function by Rob Nichols and Dennis Shea:\n",
    "# https://www.ncl.ucar.edu/Document/Functions/Contributed/pattern_cor.shtml\n",
    "\n",
    "# This function calculates the centered or uncentered\n",
    "# pattern correlation between two arrays of two dimensions\n",
    "# lat x lon\n",
    "\n",
    "# centered pattern correlation means the values are converted\n",
    "# to anomalies relative to an area-weighted mean before\n",
    "# computing the correlation\n",
    "\n",
    "# uncentered pattern correlation means the input values are\n",
    "# left alone before the pattern correlation is computed\n",
    "\n",
    "# the function takes inputs of x and y (the two grids)\n",
    "# wgt for the area-weights, and centered as 0 for uncentered\n",
    "# and 1 for centered\n",
    "# wgt can only be 1.0 in this implementation!\n",
    "\n",
    "# This function assumes that the inputs are given as xarray DataArrays\n",
    "# or numpy arrays\n",
    "\n",
    "# finally, this function assumes numpy is installed\n",
    "\n",
    "\n",
    "def pattern_cor(x, y, wgt, centered):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    #print(\"Computing pattern correlation using\")\n",
    "    #print(\"/dartfs-hpc/rc/lab/C/CMIG/ccallahan/INVERSIONS/FUNCTIONS/PATTERN_CORRELATION.py\")\n",
    "\n",
    "    # Check for input data errors\n",
    "\n",
    "    if ((np.amin(x)==0) & (np.amax(x)==0)):\n",
    "        print(\"PATTERN_COR FAILED: All values of first input grid are 0\")\n",
    "        return\n",
    "\n",
    "    if ((np.amin(y)==0) & (np.amax(y)==0)):\n",
    "        print(\"PATTERN_COR FAILED: All values of second input grid are 0\")\n",
    "        return\n",
    "\n",
    "    if (len(x.shape) != 2):\n",
    "        print(\"PATTERN_COR FAILED: First input grid must have 2 dimensions, not \"+str(x.shape))\n",
    "        return\n",
    "\n",
    "    if (len(y.shape) != 2):\n",
    "        print(\"PATTERN_COR FAILED: Second input grid must have 2 dimensions, not \"+str(y.shape))\n",
    "        return\n",
    "\n",
    "    if (y.shape != x.shape):\n",
    "        print(\"PATTERN_COR FAILED: First input grid has shape \"+str(x.shape)+\" and second input grid has shape \"+str(y.shape))\n",
    "        return\n",
    "\n",
    "    # Compute pattern correlation\n",
    "\n",
    "    if centered == 1: # centered pattern correlation\n",
    "\n",
    "        sumwgt = np.nansum(wgt)\n",
    "        xAvgArea = (np.nansum(x*wgt))/np.nansum(wgt)\n",
    "        yAvgArea = (np.nansum(y*wgt))/np.nansum(wgt)\n",
    "\n",
    "        xAnom = x - xAvgArea\n",
    "        yAnom = y - yAvgArea\n",
    "\n",
    "        xyCov = np.nansum(wgt*xAnom*yAnom)\n",
    "        xAnom2 = np.nansum(wgt*(np.square(xAnom)))\n",
    "        yAnom2 = np.nansum(wgt*(np.square(yAnom)))\n",
    "\n",
    "    else: # uncentered pattern correlation\n",
    "\n",
    "        xyCov = np.nansum(wgt*x*y)\n",
    "        xAnom2 = np.nansum(wgt*(np.square(x)))\n",
    "        yAnom2 = np.nansum(wgt*(np.square(y)))\n",
    "\n",
    "    r = xyCov/(np.sqrt(xAnom2)*np.sqrt(yAnom2))\n",
    "\n",
    "    return(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detrending function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_detrend_3d(data,order):\n",
    "    \n",
    "    # numpy required\n",
    "    # removes trend of order \"order\" from the first axis of 3-d data\n",
    "    \n",
    "    new_data = np.zeros((data.shape[0],data.shape[1],data.shape[2]))\n",
    "    \n",
    "    x = np.arange(1,(data.shape[0])+1,1)\n",
    "    \n",
    "    dim1 = data.shape[1]\n",
    "    dim2 = data.shape[2]\n",
    "    \n",
    "    for jj in np.arange(0,dim1,1):\n",
    "        for kk in np.arange(0,dim2,1):\n",
    "            subset = data[:,jj,kk]\n",
    "            model = np.polyfit(x,subset,order)\n",
    "            predicted = np.polyval(model,x)\n",
    "            new_data[:,jj,kk] = subset - predicted\n",
    "            \n",
    "    return(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(directory, extension):\n",
    "    \n",
    "    # Credit to: http://www.martinbroadhurst.com/listing-all-files-in-a-directory-with-a-certain-extension-in-python.html\n",
    "    # Requires os.listdir\n",
    "    \n",
    "    return (f for f in os.listdir(directory) if f.endswith(extension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonmin = 140\n",
    "lonmax = 280\n",
    "latmin = -15\n",
    "latmax = 15\n",
    "latctr = ((latmax-latmin)/2) + latmin\n",
    "lonctr = ((lonmax-lonmin)/2) + lonmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GISSE2R_abrupt4x\n",
      "0.9515640735626221\n",
      "HadCM3L_abrupt2x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9453275493534204\n",
      "HadCM3L_abrupt4x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8619692598851512\n",
      "HadCM3L_abrupt6x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8190450885153512\n",
      "HadCM3L_abrupt8x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795099663226961\n",
      "IPSLCM5A_abrupt4x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9573598504066467\n",
      "MIROC32_1pct2x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9779157042503357\n",
      "MIROC32_1pct4x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9253498315811157\n",
      "MPIESM12_abrupt2x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99221271276474\n",
      "MPIESM12_abrupt4x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9700103402137756\n",
      "MPIESM12_abrupt8x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/dartfs-hpc/rc/home/y/f003k8y/.conda/envs/ccallahan/lib/python3.7/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9026506543159485\n"
     ]
    }
   ],
   "source": [
    "patcors = np.zeros(len(modelnames_file))\n",
    "\n",
    "for i in np.arange(10,len(modelnames_file),1):\n",
    "    \n",
    "    model, exp = modelnames_file[i].split(\"_\")\n",
    "    print(modelnames_file[i])\n",
    "    \n",
    "    if modelnames_file[i] == \"MPIESM11_abrupt4x\":\n",
    "        fname_exp_start = \"tas_ann_\"+modelnames_file[i]\n",
    "        fname_control_start = \"tas_ann_\"+model+\"_control\"\n",
    "    else:\n",
    "        fname_exp_start = \"tas_mon_\"+modelnames_file[i]\n",
    "        fname_control_start = \"tas_mon_\"+model+\"_control\"\n",
    "    \n",
    "    fname_exp = [f for f in os.listdir(loc_tas) if f.startswith(fname_exp_start)][0]\n",
    "    fname_control = [f for f in os.listdir(loc_tas) if f.startswith(fname_control_start)][0]\n",
    "    \n",
    "    n1, n2, n3, n4, year_exp_nc = fname_exp.split(\"_\")\n",
    "    n1, n2, n3, n4, year_control_nc = fname_control.split(\"_\")\n",
    "    year_exp, nc = year_exp_nc.split(\".\")\n",
    "    year_control, nc = year_control_nc.split(\".\")\n",
    "    \n",
    "    \n",
    "    model_decode_times_list = [\"MIROC32\",\"MPIESM12\",\"MPIESM11\"]\n",
    "    if model in model_decode_times_list:\n",
    "        tas_exp_global = xr.DataArray(xr.open_dataset(loc_tas+fname_exp,decode_times=False).data_vars[\"tas\"]) #.loc[:,lat_mins[j]:lat_maxs[j],lon_mins[j]:lon_maxs[j]])\n",
    "        tas_control_global = xr.DataArray(xr.open_dataset(loc_tas+fname_control,decode_times=False).data_vars[\"tas\"]) #.loc[:,lat_mins[j]:lat_maxs[j],lon_mins[j]:lon_maxs[j]])\n",
    "    else:\n",
    "        tas_exp_global = xr.DataArray(xr.open_dataset(loc_tas+fname_exp).data_vars[\"tas\"]) #.loc[:,lat_mins[j]:lat_maxs[j],lon_mins[j]:lon_maxs[j]])\n",
    "        tas_control_global = xr.DataArray(xr.open_dataset(loc_tas+fname_control).data_vars[\"tas\"]) #.loc[:,lat_mins[j]:lat_maxs[j],lon_mins[j]:lon_maxs[j]])\n",
    "            \n",
    "    if model == \"HadCM3L\":\n",
    "        lat_tas = tas_exp_global.coords[\"latitude_1\"]\n",
    "        lon_tas = tas_exp_global.coords[\"longitude_1\"]\n",
    "    else:\n",
    "        lat_tas = tas_exp_global.coords[\"lat\"]\n",
    "        lon_tas = tas_exp_global.coords[\"lon\"]\n",
    "    \n",
    "    latshape = len(lat_tas)\n",
    "    lonshape = len(lon_tas)\n",
    "    \n",
    "    nino34_ctrl_anom = xr.DataArray(xr.open_dataset(loc_nino34+\"nino34_\"+model+\"_control_anom_detrend2.nc\").data_vars[\"nino34\"])\n",
    "    nino34_forced_anom = xr.DataArray(xr.open_dataset(loc_nino34+\"nino34_\"+model+\"_\"+exp+\"_anom_detrend2.nc\").data_vars[\"nino34\"])\n",
    "    \n",
    "    # limit to pacific tas\n",
    "    if (lat_tas[0] > lat_tas[10]):\n",
    "        tas_pacific_exp = tas_exp_global.loc[:,latmax:latmin,lonmin:lonmax]\n",
    "        tas_pacific_control = tas_control_global.loc[:,latmax:latmin,lonmin:lonmax]\n",
    "    else:\n",
    "        tas_pacific_exp = tas_exp_global.loc[:,latmin:latmax,lonmin:lonmax]\n",
    "        tas_pacific_control = tas_control_global.loc[:,latmin:latmax,lonmin:lonmax]\n",
    "    \n",
    "    # Calculate 75th/90th percentiles\n",
    "    nino34_75_f = np.percentile(nino34_forced_anom.values,75)\n",
    "    nino34_90_f = np.percentile(nino34_forced_anom.values,90)\n",
    "    nino34_75_c = np.percentile(nino34_ctrl_anom.values,75)\n",
    "    nino34_90_c = np.percentile(nino34_ctrl_anom.values,90)\n",
    "    \n",
    "    # Calculate composites\n",
    "    pacific_composite_f = tas_pacific_exp[((nino34_forced_anom.values >= nino34_75_f) & (nino34_forced_anom.values <= nino34_90_f)),:,:].mean(dim=\"time\")\n",
    "    pacific_composite_c = tas_pacific_control[((nino34_ctrl_anom.values >= nino34_75_c) & (nino34_ctrl_anom.values <= nino34_90_c)),:,:].mean(dim=\"time\")\n",
    "    \n",
    "    # create spatial anomaly (i.e. remove spatial mean from composite)\n",
    "    composite_f_centered = pacific_composite_f.values - pacific_composite_f.values.mean(axis=(0,1))\n",
    "    composite_c_centered = pacific_composite_c.values - pacific_composite_c.values.mean(axis=(0,1))\n",
    "    \n",
    "    # take pattern correlation\n",
    "    patcors[i] = pattern_cor(composite_f_centered,composite_c_centered,1.0,0)\n",
    "    \n",
    "    print(patcors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCSM3_abrupt2x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "CCSM3_abrupt4x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "CCSM3_abrupt8x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "CESM1-0-4_abrupt2x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "CESM1-0-4_abrupt4x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "CESM1-0-4_abrupt8x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "GFDL-CM3_1pct2x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "GFDL-ESM-2M_1pct2x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "GISS-E2R_1pct4x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "GISS-E2R_abrupt4x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "HadCM3L_abrupt2x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "HadCM3L_abrupt4x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "HadCM3L_abrupt6x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "HadCM3L_abrupt8x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "IPSL_abrupt4x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "MIROC_1pct2x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "MIROC_1pct4x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "MPI-ESM-12_abrupt2x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "MPI-ESM-12_abrupt4x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "MPI-ESM-12_abrupt8x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n",
      "MPI-ESM-LR_abrupt4x\n",
      "Calculating anomalies...\n",
      "Calculating composites...\n"
     ]
    }
   ],
   "source": [
    "ordr = 2\n",
    "\n",
    "patcors = np.zeros(len(ex1))\n",
    "\n",
    "lonmin = 140\n",
    "lonmax = 280\n",
    "latmin = -15\n",
    "latmax = 15\n",
    "latctr = ((latmax-latmin)/2) + latmin\n",
    "lonctr = ((lonmax-lonmin)/2) + lonmin\n",
    "\n",
    "lonmin_nino34 = 190\n",
    "lonmax_nino34 = 240\n",
    "latmin_nino34 = -5\n",
    "latmax_nino34 = 5\n",
    "\n",
    "# Loop through all models, read in Pacific temps, calculate Nino3.4 index, calculate composite and patcor\n",
    "# and if it's the desired example, save for later\n",
    "#example_model = \"CESM1-0-4_abrupt4x\"\n",
    "example_model = \"GISS-E2R_abrupt4x\"\n",
    "\n",
    "for i in np.arange(0,len(ex1),1):\n",
    "    \n",
    "    print(ex1[i])\n",
    "    \n",
    "    fname = xr.open_dataset(loc_in_pacific_temp+\"pacific.temps.\"+ex1[i]+\".nc\")\n",
    "    pacific_temps_f = xr.DataArray(fname.data_vars[\"pacific_temp_f\"]).values\n",
    "    pacific_temps_c = xr.DataArray(fname.data_vars[\"pacific_temp_c\"]).values\n",
    "    \n",
    "    lat = xr.DataArray(fname.data_vars[\"pacific_lat\"])\n",
    "    lon = xr.DataArray(fname.data_vars[\"pacific_lon\"])\n",
    "    \n",
    "    \n",
    "    # assign time array\n",
    "    n_months_c = pacific_temps_c.shape[0]\n",
    "    n_months_f = pacific_temps_f.shape[0]\n",
    "    \n",
    "    time_ctrl = xr.cftime_range(start='0001', periods=n_months_c, freq='M')\n",
    "    time_forced = xr.cftime_range(start='0001', periods=n_months_f, freq='M')\n",
    "    \n",
    "    pacific_temps_c_xr = xr.DataArray(pacific_temps_c,coords=[time_ctrl,lat,lon],dims=[\"time\",\"lat\",\"lon\"])\n",
    "    pacific_temps_f_xr = xr.DataArray(pacific_temps_f,coords=[time_forced,lat,lon],dims=[\"time\",\"lat\",\"lon\"])\n",
    "    \n",
    "    # calculate anomalies\n",
    "    print(\"Calculating anomalies...\")\n",
    "    \n",
    "    pacific_ctrl_detrend = xr.DataArray(signal.detrend(pacific_temps_c_xr,axis=0),coords=[pacific_temps_c_xr.coords[\"time\"],lat,lon],dims=[\"time\",\"lat\",\"lon\"])\n",
    "    pacific_ctrl_anom = pacific_ctrl_detrend.groupby(\"time.month\") - (pacific_ctrl_detrend.groupby(\"time.month\").mean(dim=\"time\"))\n",
    "    \n",
    "    if runtype[i] == \"abrupt\":\n",
    "        \n",
    "        # calculate anomalies by splitting into fast/transient/equilibrium periods, detrend, find anomalies, re-concatenate\n",
    "        pacific_fast = pacific_temps_f_xr[0:(50*12),:,:]\n",
    "        pacific_transient = pacific_temps_f_xr[(50*12):(150*12),:,:]\n",
    "        pacific_eq = pacific_temps_f_xr[(150*12):,:,:]\n",
    "\n",
    "        pacific_fast_detrend = xr.DataArray(custom_detrend_3d(pacific_fast,ordr),coords=[pacific_fast.coords[\"time\"],lat,lon],dims=[\"time\",\"lat\",\"lon\"])\n",
    "        pacific_fast_anom = pacific_fast_detrend.groupby(\"time.month\") - (pacific_fast_detrend.groupby(\"time.month\").mean(dim=\"time\"))\n",
    "\n",
    "        pacific_transient_detrend = xr.DataArray(custom_detrend_3d(pacific_transient,ordr),coords=[pacific_transient.coords[\"time\"],lat,lon],dims=[\"time\",\"lat\",\"lon\"])\n",
    "        pacific_transient_anom = pacific_transient_detrend.groupby(\"time.month\") - (pacific_transient_detrend.groupby(\"time.month\").mean(dim=\"time\"))\n",
    "\n",
    "        pacific_eq_detrend = xr.DataArray(custom_detrend_3d(pacific_eq,ordr),coords=[pacific_eq.coords[\"time\"],lat,lon],dims=[\"time\",\"lat\",\"lon\"])\n",
    "        pacific_eq_anom = pacific_eq_detrend.groupby(\"time.month\") - (pacific_eq_detrend.groupby(\"time.month\").mean(dim=\"time\"))\n",
    "\n",
    "        pacific_forced_anom = xr.concat([pacific_fast_anom,pacific_transient_anom,pacific_eq_anom],dim=\"time\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # calculate anomalies by splitting into transient/equilibrium periods, detrend, find anomalies, re-concatenate\n",
    "        pacific_transient = pacific_temps_f_xr[0:(140*12),:,:]\n",
    "        pacific_eq = pacific_temps_f_xr[(140*12):,:,:]\n",
    "        \n",
    "        pacific_transient_detrend = xr.DataArray(custom_detrend_3d(pacific_transient,ordr),coords=[pacific_transient.coords[\"time\"],lat,lon],dims=[\"time\",\"lat\",\"lon\"])\n",
    "        pacific_transient_anom = pacific_transient_detrend.groupby(\"time.month\") - (pacific_transient_detrend.groupby(\"time.month\").mean(dim=\"time\"))\n",
    "\n",
    "        pacific_eq_detrend = xr.DataArray(custom_detrend_3d(pacific_eq,ordr),coords=[pacific_eq.coords[\"time\"],lat,lon],dims=[\"time\",\"lat\",\"lon\"])\n",
    "        pacific_eq_anom = pacific_eq_detrend.groupby(\"time.month\") - (pacific_eq_detrend.groupby(\"time.month\").mean(dim=\"time\"))\n",
    "\n",
    "        pacific_forced_anom = xr.concat([pacific_transient_anom,pacific_eq_anom],dim=\"time\")\n",
    "    \n",
    "    print(\"Calculating composites...\")\n",
    "    # Calculate Nino3.4\n",
    "    nino34_f = pacific_forced_anom.loc[:,latmin_nino34:latmax_nino34,lonmin_nino34:lonmax_nino34].mean(dim=[\"lat\",\"lon\"])\n",
    "    nino34_c = pacific_ctrl_anom.loc[:,latmin_nino34:latmax_nino34,lonmin_nino34:lonmax_nino34].mean(dim=[\"lat\",\"lon\"])\n",
    "    \n",
    "    # Calculate 75th/90th percentiles\n",
    "    nino34_75_f = np.percentile(nino34_f,75)\n",
    "    nino34_90_f = np.percentile(nino34_f,90)\n",
    "    nino34_75_c = np.percentile(nino34_c,75)\n",
    "    nino34_90_c = np.percentile(nino34_c,90)\n",
    "    \n",
    "    # Calculate composites\n",
    "    pacific_composite_f_full = pacific_forced_anom[((nino34_f >= nino34_75_f) & (nino34_f <= nino34_90_f)),:,:].mean(dim=\"time\")\n",
    "    pacific_composite_c_full = pacific_ctrl_anom[((nino34_c >= nino34_75_c) & (nino34_c <= nino34_90_c)),:,:].mean(dim=\"time\")\n",
    "    \n",
    "    pacific_composite_f = pacific_composite_f_full.loc[latmin:latmax,lonmin:lonmax]\n",
    "    pacific_composite_c = pacific_composite_c_full.loc[latmin:latmax,lonmin:lonmax]\n",
    "    \n",
    "    # Pattern correlation\n",
    "    patcors[i] = pattern_cor(pacific_composite_f.values,pacific_composite_c.values,1.0,0)\n",
    "    \n",
    "    # save example composites\n",
    "    if ex1[i] == example_model:\n",
    "        example_composite_f = pacific_composite_f\n",
    "        example_composite_c = pacific_composite_c\n",
    "        lat_example = pacific_composite_f.coords[\"lat\"]\n",
    "        lon_example = pacific_composite_f.coords[\"lon\"]\n",
    "    \n",
    "\n",
    "#### OLD\n",
    "\n",
    "#f_cesm_ctrl = xr.open_dataset(\"./RAW_DATA/tas_mon_CESM104_control_1000.nc\")\n",
    "#f_cesm_a = xr.open_dataset(\"./RAW_DATA/tas_mon_CESM104_abrupt4x_5300.nc\")\n",
    "\n",
    "#f_mpi_ctrl = xr.open_dataset(\"tas_mon_MPIESM12_control_1237.nc\")\n",
    "#f_mpi_a = xr.open_dataset(\"tas_mon_MPIESM12_abrupt4x_1000.nc\")\n",
    "\n",
    "#f_giss_a = xr.open_dataset(\"./RAW_DATA/tas_Amon_GISS-E2R_abrupt4x_LongRunMIP_000101-500112.nc\")\n",
    "\n",
    "#f_had_a = xr.open_dataset(\"./RAW_DATA/tas_Amon_HadCM3L_abrupt4x_LongRunMIP_000101-100012.nc\")\n",
    "\n",
    "#tas_cesm_ctrl = (xr.DataArray(f_cesm_ctrl.data_vars[\"tas\"]).loc[:,latmin:latmax,lonmin:lonmax])\n",
    "#tas_mpi_ctrl = (xr.DataArray(f_mpi_ctrl.data_vars[\"tas\"]).loc[:,latmin:latmax,lonmin:lonmax])\n",
    "#latc = f_cesm_ctrl.coords[\"lat\"].loc[latmin:latmax]\n",
    "#lonc = f_cesm_ctrl.coords[\"lon\"].loc[lonmin:lonmax]\n",
    "\n",
    "#lath = f_had_a.coords[\"latitude_1\"].loc[latmax:latmin]\n",
    "#lonh = f_had_a.coords[\"longitude_1\"].loc[lonmin:lonmax]\n",
    "#latg = f_giss_a.coords[\"lat\"].loc[latmin:latmax]\n",
    "#long = f_giss_a.coords[\"lon\"].loc[lonmin:lonmax]\n",
    "#latm = f_mpi_ctrl.coords[\"lat\"].loc[latmin:latmax]\n",
    "#lonm = f_mpi_ctrl.coords[\"lon\"].loc[lonmin:lonmax]\n",
    "\n",
    "#std_cesm_ctrl = tas_cesm_ctrl.std(dim=\"time\")\n",
    "#std_mpi_ctrl = tas_mpi_ctrl.std(dim=\"time\")\n",
    "\n",
    "#tas_cesm = xr.DataArray(f_cesm_a.data_vars[\"tas\"]).loc[:,latmin:latmax,lonmin:lonmax]\n",
    "#tas_had = xr.DataArray(f_had_a.data_vars[\"temp_1\"]).loc[:,latmax:latmin,lonmin:lonmax]\n",
    "#tas_giss = xr.DataArray(f_giss_a.data_vars[\"tas\"]).loc[:,latmin:latmax,lonmin:lonmax]\n",
    "#tas_mpi = xr.DataArray(f_mpi_a.data_vars[\"tas\"]).loc[:,latmin:latmax,lonmin:lonmax]\n",
    "\n",
    "# Calculate standard deviation\n",
    "#std_cesm_a = tas_cesm_filtered[(150*12):(tas_cesm.shape[0]*12)-1,:,:].std(dim=\"time\")\n",
    "#std_had_a = tas_had_filtered[(150*12):(tas_had.shape[0]*12)-1,:,:].std(dim=\"time\")\n",
    "#std_giss_a = tas_giss[(150*12):(tas_giss.shape[0]*12)-1,:,:].std(dim=\"time\")\n",
    "#std_mpi_a = tas_mpi[(150*12):(tas_mpi.shape[0]*12)-1,:,:].std(dim=\"time\")\n",
    "\n",
    "#cesm = ((std_cesm_a - std_cesm_ctrl)/std_cesm_ctrl)*100\n",
    "#mpi = ((std_mpi_a - std_mpi_ctrl)/std_mpi_ctrl)*100\n",
    "\n",
    "#cesm = std_cesm_a\n",
    "#giss = std_giss_a\n",
    "#mpi = std_mpi_a\n",
    "\n",
    "#lat1 = latc\n",
    "#lon1 = lonc\n",
    "#lat2 = lath\n",
    "#lon2 = lonh\n",
    "#model1 = cesm\n",
    "#model2 = std_had_a\n",
    "#model1name = \"CESM1.0.4 abrupt4x\"\n",
    "#model2name = \"HadCM3L abrupt4x\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ccallahan",
   "language": "python",
   "name": "ccallahan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
